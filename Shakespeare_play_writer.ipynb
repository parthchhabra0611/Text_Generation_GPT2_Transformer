{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare play-writer",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eSPxalhg2y6",
        "colab_type": "text"
      },
      "source": [
        "##GPT2 Intro:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oyiZkvoH1QF",
        "colab_type": "text"
      },
      "source": [
        "Developed by OpenAI, GPT2 is a large-scale transformer-based language model that is pre-trained on a large corpus of text: 8 million high-quality webpages. It results in competitive performance on multiple language tasks using only the pre-trained knowledge without explicitly training on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYEjLqp4HxTr",
        "colab_type": "text"
      },
      "source": [
        "“GPT-2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the “zero-shot” setting. GPT-2 outperforms models trained on domain-specific data sets (e.g. Wikipedia, news, books) when evaluated on those same data sets.” – Open AI team."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBugpsNLmSuj",
        "colab_type": "text"
      },
      "source": [
        "We will use it to make a Shakespeare's play-writer and generate new text based on Shakespeare's text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OFVW_HqhXcu",
        "colab_type": "text"
      },
      "source": [
        "## Installing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3GHFwN6hCo5",
        "colab_type": "text"
      },
      "source": [
        "Firstly, we'll install the transformers package using pip and the link to the github repo https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE243dPaG1Kp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "8c9f05f5-4cc9-421b-f2f9-8ccd4a837454"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-ece2hvq1\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-ece2hvq1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (1.18.5)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 48.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2) (0.7)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2) (2020.6.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2) (0.16.0)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.0.2-cp36-none-any.whl size=879620 sha256=108df0183d9c5070813b9c08fbbc98492c0450bfe239c877baac94f2b0f450f0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9ezoin3_/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=23854d3195936f7b2ed543a83e8d52adc687faf3f76b46b137ebbac4d4d155aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy3afqPKWw3B",
        "colab_type": "text"
      },
      "source": [
        "Upgraded version of pyarrow is required while fine-tuning as the previous version tends to show errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPz3a8khG2E0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "cfc7a13e-859c-44fe-d29d-d595ef3a575e"
      },
      "source": [
        "!pip install --upgrade pyarrow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarrow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/99/0a605f016121ca314d1469dc9069e4978395bc46fda40f73099d90ad3ba4/pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 203kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.18.5)\n",
            "Installing collected packages: pyarrow\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed pyarrow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnwcMpKZhwB_",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Shakespeare's play text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNLjCxdjYElz",
        "colab_type": "text"
      },
      "source": [
        "The text is taken from here: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeh9Bdhih1k1",
        "colab_type": "text"
      },
      "source": [
        "We will use the !wget command to download the text and save it in the input directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZwMfmmCG2J7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "7d60f9bb-5c42-4abd-aef8-72c093866037"
      },
      "source": [
        "# Download the Shakespeare's text.\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-28 22:07:42--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-08-28 22:07:42 (16.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQF9pkDEW85C",
        "colab_type": "text"
      },
      "source": [
        "Making an output directory to save the tokenizer and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BudonDHOG2Se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4EbYthhXFCH",
        "colab_type": "text"
      },
      "source": [
        "Loading the run_language_modeling.py using !wget command which will be used for fine-tuning on our custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4CQNIZdHfgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "ae11cc84-7548-4d00-cda1-2f7236b3091a"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-28 22:09:09--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/language-modeling/run_language_modeling.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11192 (11K) [text/plain]\n",
            "Saving to: ‘run_language_modeling.py’\n",
            "\n",
            "\rrun_language_modeli   0%[                    ]       0  --.-KB/s               \rrun_language_modeli 100%[===================>]  10.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-08-28 22:09:10 (104 MB/s) - ‘run_language_modeling.py’ saved [11192/11192]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjpI6jMAXQra",
        "colab_type": "text"
      },
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvWDUlO8iIfl",
        "colab_type": "text"
      },
      "source": [
        "With the packages installed and the text data loaded, it is time that we fine tune gpt2 for generating texts similar to the play text downloaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSQnTv70G2WM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bee4fafd-f9cc-4288-ea46-b1808a809282"
      },
      "source": [
        "!python run_language_modeling.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file='/content/input.txt' \\\n",
        "    --per_gpu_train_batch_size=1 \\\n",
        "    --save_steps=-1 \\\n",
        "    --num_train_epochs=2"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "2020-08-28 22:09:16.752154: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "TensorFlow version 2.3.0 available.\n",
            "PyTorch: setting up devices\n",
            "08/28/2020 22:09:18 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "08/28/2020 22:09:18 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=1, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Aug28_22-09-18_daa890b659ff', logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True)\n",
            "08/28/2020 22:09:18 - INFO - filelock -   Lock 139890558701296 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpjjdbzcvs\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "08/28/2020 22:09:19 - INFO - filelock -   Lock 139890558701296 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "08/28/2020 22:09:19 - INFO - filelock -   Lock 139890160035600 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp_qiz1vgi\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "08/28/2020 22:09:20 - INFO - filelock -   Lock 139890160035600 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "08/28/2020 22:09:20 - INFO - filelock -   Lock 139890159989648 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8m2dl_5d\n",
            "storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "08/28/2020 22:09:21 - INFO - filelock -   Lock 139890159989648 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_auto.py:821: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "08/28/2020 22:09:21 - INFO - filelock -   Lock 139890159991216 acquired on /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "https://cdn.huggingface.co/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp0fnzg8ly\n",
            "storing https://cdn.huggingface.co/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "creating metadata file for /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "08/28/2020 22:09:31 - INFO - filelock -   Lock 139890159991216 released on /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1321: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
            "  FutureWarning,\n",
            "08/28/2020 22:09:36 - INFO - filelock -   Lock 139889739148928 acquired on /content/cached_lm_GPT2Tokenizer_1024_input.txt.lock\n",
            "Creating features from dataset file at /content\n",
            "Saving features into cached file /content/cached_lm_GPT2Tokenizer_1024_input.txt [took 0.008 s]\n",
            "08/28/2020 22:09:37 - INFO - filelock -   Lock 139889739148928 released on /content/cached_lm_GPT2Tokenizer_1024_input.txt.lock\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:245: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
            "  FutureWarning,\n",
            "You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
            "To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "***** Running training *****\n",
            "  Num examples = 330\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 660\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/330 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/330 [00:00<03:58,  1.38it/s]\u001b[A\n",
            "Iteration:   1% 2/330 [00:01<03:20,  1.63it/s]\u001b[A\n",
            "Iteration:   1% 3/330 [00:01<02:56,  1.85it/s]\u001b[A\n",
            "Iteration:   1% 4/330 [00:01<02:42,  2.01it/s]\u001b[A\n",
            "Iteration:   2% 5/330 [00:02<02:29,  2.17it/s]\u001b[A\n",
            "Iteration:   2% 6/330 [00:02<02:21,  2.30it/s]\u001b[A\n",
            "Iteration:   2% 7/330 [00:02<02:15,  2.38it/s]\u001b[A\n",
            "Iteration:   2% 8/330 [00:03<02:10,  2.46it/s]\u001b[A\n",
            "Iteration:   3% 9/330 [00:03<02:08,  2.51it/s]\u001b[A\n",
            "Iteration:   3% 10/330 [00:04<02:06,  2.53it/s]\u001b[A\n",
            "Iteration:   3% 11/330 [00:04<02:04,  2.56it/s]\u001b[A\n",
            "Iteration:   4% 12/330 [00:04<02:02,  2.59it/s]\u001b[A\n",
            "Iteration:   4% 13/330 [00:05<02:02,  2.59it/s]\u001b[A\n",
            "Iteration:   4% 14/330 [00:05<02:01,  2.60it/s]\u001b[A\n",
            "Iteration:   5% 15/330 [00:06<02:00,  2.62it/s]\u001b[A\n",
            "Iteration:   5% 16/330 [00:06<01:59,  2.62it/s]\u001b[A\n",
            "Iteration:   5% 17/330 [00:06<01:59,  2.62it/s]\u001b[A\n",
            "Iteration:   5% 18/330 [00:07<01:59,  2.62it/s]\u001b[A\n",
            "Iteration:   6% 19/330 [00:07<01:58,  2.62it/s]\u001b[A\n",
            "Iteration:   6% 20/330 [00:07<01:58,  2.62it/s]\u001b[A\n",
            "Iteration:   6% 21/330 [00:08<01:58,  2.62it/s]\u001b[A\n",
            "Iteration:   7% 22/330 [00:08<01:57,  2.62it/s]\u001b[A\n",
            "Iteration:   7% 23/330 [00:09<01:57,  2.62it/s]\u001b[A\n",
            "Iteration:   7% 24/330 [00:09<01:56,  2.62it/s]\u001b[A\n",
            "Iteration:   8% 25/330 [00:09<01:56,  2.61it/s]\u001b[A\n",
            "Iteration:   8% 26/330 [00:10<01:56,  2.61it/s]\u001b[A\n",
            "Iteration:   8% 27/330 [00:10<01:56,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 28/330 [00:10<01:56,  2.60it/s]\u001b[A\n",
            "Iteration:   9% 29/330 [00:11<01:55,  2.60it/s]\u001b[A\n",
            "Iteration:   9% 30/330 [00:11<01:55,  2.59it/s]\u001b[A\n",
            "Iteration:   9% 31/330 [00:12<01:55,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 32/330 [00:12<01:54,  2.59it/s]\u001b[A\n",
            "Iteration:  10% 33/330 [00:12<01:54,  2.60it/s]\u001b[A\n",
            "Iteration:  10% 34/330 [00:13<01:54,  2.59it/s]\u001b[A\n",
            "Iteration:  11% 35/330 [00:13<01:53,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 36/330 [00:14<01:53,  2.60it/s]\u001b[A\n",
            "Iteration:  11% 37/330 [00:14<01:52,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 38/330 [00:14<01:52,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 39/330 [00:15<01:51,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 40/330 [00:15<01:51,  2.60it/s]\u001b[A\n",
            "Iteration:  12% 41/330 [00:16<01:51,  2.59it/s]\u001b[A\n",
            "Iteration:  13% 42/330 [00:16<01:50,  2.60it/s]\u001b[A\n",
            "Iteration:  13% 43/330 [00:16<01:50,  2.59it/s]\u001b[A\n",
            "Iteration:  13% 44/330 [00:17<01:50,  2.59it/s]\u001b[A\n",
            "Iteration:  14% 45/330 [00:17<01:49,  2.60it/s]\u001b[A\n",
            "Iteration:  14% 46/330 [00:17<01:49,  2.60it/s]\u001b[A\n",
            "Iteration:  14% 47/330 [00:18<01:48,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 48/330 [00:18<01:48,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 49/330 [00:19<01:48,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 50/330 [00:19<01:47,  2.60it/s]\u001b[A\n",
            "Iteration:  15% 51/330 [00:19<01:47,  2.59it/s]\u001b[A\n",
            "Iteration:  16% 52/330 [00:20<01:47,  2.59it/s]\u001b[A\n",
            "Iteration:  16% 53/330 [00:20<01:46,  2.59it/s]\u001b[A\n",
            "Iteration:  16% 54/330 [00:21<01:46,  2.58it/s]\u001b[A\n",
            "Iteration:  17% 55/330 [00:21<01:46,  2.59it/s]\u001b[A\n",
            "Iteration:  17% 56/330 [00:21<01:45,  2.59it/s]\u001b[A\n",
            "Iteration:  17% 57/330 [00:22<01:45,  2.58it/s]\u001b[A\n",
            "Iteration:  18% 58/330 [00:22<01:45,  2.58it/s]\u001b[A\n",
            "Iteration:  18% 59/330 [00:22<01:45,  2.58it/s]\u001b[A\n",
            "Iteration:  18% 60/330 [00:23<01:44,  2.58it/s]\u001b[A\n",
            "Iteration:  18% 61/330 [00:23<01:44,  2.57it/s]\u001b[A\n",
            "Iteration:  19% 62/330 [00:24<01:44,  2.57it/s]\u001b[A\n",
            "Iteration:  19% 63/330 [00:24<01:43,  2.57it/s]\u001b[A\n",
            "Iteration:  19% 64/330 [00:24<01:43,  2.57it/s]\u001b[A\n",
            "Iteration:  20% 65/330 [00:25<01:43,  2.57it/s]\u001b[A\n",
            "Iteration:  20% 66/330 [00:25<01:42,  2.57it/s]\u001b[A\n",
            "Iteration:  20% 67/330 [00:26<01:42,  2.57it/s]\u001b[A\n",
            "Iteration:  21% 68/330 [00:26<01:42,  2.57it/s]\u001b[A\n",
            "Iteration:  21% 69/330 [00:26<01:41,  2.56it/s]\u001b[A\n",
            "Iteration:  21% 70/330 [00:27<01:41,  2.57it/s]\u001b[A\n",
            "Iteration:  22% 71/330 [00:27<01:41,  2.56it/s]\u001b[A\n",
            "Iteration:  22% 72/330 [00:28<01:40,  2.56it/s]\u001b[A\n",
            "Iteration:  22% 73/330 [00:28<01:40,  2.56it/s]\u001b[A\n",
            "Iteration:  22% 74/330 [00:28<01:40,  2.56it/s]\u001b[A\n",
            "Iteration:  23% 75/330 [00:29<01:40,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 76/330 [00:29<01:40,  2.53it/s]\u001b[A\n",
            "Iteration:  23% 77/330 [00:29<01:39,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 78/330 [00:30<01:39,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 79/330 [00:30<01:38,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 80/330 [00:31<01:38,  2.55it/s]\u001b[A\n",
            "Iteration:  25% 81/330 [00:31<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 82/330 [00:31<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 83/330 [00:32<01:37,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 84/330 [00:32<01:36,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 85/330 [00:33<01:36,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 86/330 [00:33<01:36,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 87/330 [00:33<01:35,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 88/330 [00:34<01:35,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 89/330 [00:34<01:35,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 90/330 [00:35<01:34,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 91/330 [00:35<01:34,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 92/330 [00:35<01:34,  2.52it/s]\u001b[A\n",
            "Iteration:  28% 93/330 [00:36<01:33,  2.52it/s]\u001b[A\n",
            "Iteration:  28% 94/330 [00:36<01:33,  2.52it/s]\u001b[A\n",
            "Iteration:  29% 95/330 [00:37<01:33,  2.52it/s]\u001b[A\n",
            "Iteration:  29% 96/330 [00:37<01:32,  2.53it/s]\u001b[A\n",
            "Iteration:  29% 97/330 [00:37<01:32,  2.52it/s]\u001b[A\n",
            "Iteration:  30% 98/330 [00:38<01:32,  2.52it/s]\u001b[A\n",
            "Iteration:  30% 99/330 [00:38<01:31,  2.52it/s]\u001b[A\n",
            "Iteration:  30% 100/330 [00:39<01:31,  2.51it/s]\u001b[A\n",
            "Iteration:  31% 101/330 [00:39<01:31,  2.51it/s]\u001b[A\n",
            "Iteration:  31% 102/330 [00:39<01:30,  2.51it/s]\u001b[A\n",
            "Iteration:  31% 103/330 [00:40<01:30,  2.51it/s]\u001b[A\n",
            "Iteration:  32% 104/330 [00:40<01:30,  2.51it/s]\u001b[A\n",
            "Iteration:  32% 105/330 [00:41<01:29,  2.50it/s]\u001b[A\n",
            "Iteration:  32% 106/330 [00:41<01:29,  2.50it/s]\u001b[A\n",
            "Iteration:  32% 107/330 [00:41<01:29,  2.50it/s]\u001b[A\n",
            "Iteration:  33% 108/330 [00:42<01:28,  2.50it/s]\u001b[A\n",
            "Iteration:  33% 109/330 [00:42<01:28,  2.49it/s]\u001b[A\n",
            "Iteration:  33% 110/330 [00:43<01:28,  2.49it/s]\u001b[A\n",
            "Iteration:  34% 111/330 [00:43<01:27,  2.49it/s]\u001b[A\n",
            "Iteration:  34% 112/330 [00:43<01:27,  2.49it/s]\u001b[A\n",
            "Iteration:  34% 113/330 [00:44<01:27,  2.49it/s]\u001b[A\n",
            "Iteration:  35% 114/330 [00:44<01:26,  2.49it/s]\u001b[A\n",
            "Iteration:  35% 115/330 [00:45<01:26,  2.49it/s]\u001b[A\n",
            "Iteration:  35% 116/330 [00:45<01:25,  2.50it/s]\u001b[A\n",
            "Iteration:  35% 117/330 [00:45<01:24,  2.51it/s]\u001b[A\n",
            "Iteration:  36% 118/330 [00:46<01:24,  2.51it/s]\u001b[A\n",
            "Iteration:  36% 119/330 [00:46<01:24,  2.50it/s]\u001b[A\n",
            "Iteration:  36% 120/330 [00:47<01:23,  2.50it/s]\u001b[A\n",
            "Iteration:  37% 121/330 [00:47<01:23,  2.49it/s]\u001b[A\n",
            "Iteration:  37% 122/330 [00:47<01:23,  2.49it/s]\u001b[A\n",
            "Iteration:  37% 123/330 [00:48<01:23,  2.48it/s]\u001b[A\n",
            "Iteration:  38% 124/330 [00:48<01:22,  2.48it/s]\u001b[A\n",
            "Iteration:  38% 125/330 [00:49<01:22,  2.48it/s]\u001b[A\n",
            "Iteration:  38% 126/330 [00:49<01:22,  2.48it/s]\u001b[A\n",
            "Iteration:  38% 127/330 [00:49<01:21,  2.49it/s]\u001b[A\n",
            "Iteration:  39% 128/330 [00:50<01:21,  2.49it/s]\u001b[A\n",
            "Iteration:  39% 129/330 [00:50<01:20,  2.48it/s]\u001b[A\n",
            "Iteration:  39% 130/330 [00:51<01:20,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 131/330 [00:51<01:20,  2.47it/s]\u001b[A\n",
            "Iteration:  40% 132/330 [00:51<01:19,  2.48it/s]\u001b[A\n",
            "Iteration:  40% 133/330 [00:52<01:19,  2.48it/s]\u001b[A\n",
            "Iteration:  41% 134/330 [00:52<01:19,  2.48it/s]\u001b[A\n",
            "Iteration:  41% 135/330 [00:53<01:18,  2.47it/s]\u001b[A\n",
            "Iteration:  41% 136/330 [00:53<01:18,  2.46it/s]\u001b[A\n",
            "Iteration:  42% 137/330 [00:53<01:18,  2.47it/s]\u001b[A\n",
            "Iteration:  42% 138/330 [00:54<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  42% 139/330 [00:54<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  42% 140/330 [00:55<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  43% 141/330 [00:55<01:17,  2.45it/s]\u001b[A\n",
            "Iteration:  43% 142/330 [00:55<01:16,  2.46it/s]\u001b[A\n",
            "Iteration:  43% 143/330 [00:56<01:15,  2.47it/s]\u001b[A\n",
            "Iteration:  44% 144/330 [00:56<01:15,  2.46it/s]\u001b[A\n",
            "Iteration:  44% 145/330 [00:57<01:15,  2.46it/s]\u001b[A\n",
            "Iteration:  44% 146/330 [00:57<01:14,  2.46it/s]\u001b[A\n",
            "Iteration:  45% 147/330 [00:58<01:14,  2.46it/s]\u001b[A\n",
            "Iteration:  45% 148/330 [00:58<01:14,  2.46it/s]\u001b[A\n",
            "Iteration:  45% 149/330 [00:58<01:13,  2.45it/s]\u001b[A\n",
            "Iteration:  45% 150/330 [00:59<01:13,  2.45it/s]\u001b[A\n",
            "Iteration:  46% 151/330 [00:59<01:13,  2.45it/s]\u001b[A\n",
            "Iteration:  46% 152/330 [01:00<01:12,  2.45it/s]\u001b[A\n",
            "Iteration:  46% 153/330 [01:00<01:12,  2.45it/s]\u001b[A\n",
            "Iteration:  47% 154/330 [01:00<01:11,  2.45it/s]\u001b[A\n",
            "Iteration:  47% 155/330 [01:01<01:11,  2.44it/s]\u001b[A\n",
            "Iteration:  47% 156/330 [01:01<01:11,  2.44it/s]\u001b[A\n",
            "Iteration:  48% 157/330 [01:02<01:10,  2.44it/s]\u001b[A\n",
            "Iteration:  48% 158/330 [01:02<01:10,  2.44it/s]\u001b[A\n",
            "Iteration:  48% 159/330 [01:02<01:10,  2.43it/s]\u001b[A\n",
            "Iteration:  48% 160/330 [01:03<01:09,  2.43it/s]\u001b[A\n",
            "Iteration:  49% 161/330 [01:03<01:09,  2.43it/s]\u001b[A\n",
            "Iteration:  49% 162/330 [01:04<01:09,  2.42it/s]\u001b[A\n",
            "Iteration:  49% 163/330 [01:04<01:08,  2.43it/s]\u001b[A\n",
            "Iteration:  50% 164/330 [01:05<01:08,  2.42it/s]\u001b[A\n",
            "Iteration:  50% 165/330 [01:05<01:08,  2.42it/s]\u001b[A\n",
            "Iteration:  50% 166/330 [01:05<01:07,  2.42it/s]\u001b[A\n",
            "Iteration:  51% 167/330 [01:06<01:07,  2.42it/s]\u001b[A\n",
            "Iteration:  51% 168/330 [01:06<01:07,  2.42it/s]\u001b[A\n",
            "Iteration:  51% 169/330 [01:07<01:06,  2.42it/s]\u001b[A\n",
            "Iteration:  52% 170/330 [01:07<01:06,  2.41it/s]\u001b[A\n",
            "Iteration:  52% 171/330 [01:07<01:05,  2.41it/s]\u001b[A\n",
            "Iteration:  52% 172/330 [01:08<01:05,  2.40it/s]\u001b[A\n",
            "Iteration:  52% 173/330 [01:08<01:05,  2.39it/s]\u001b[A\n",
            "Iteration:  53% 174/330 [01:09<01:05,  2.40it/s]\u001b[A\n",
            "Iteration:  53% 175/330 [01:09<01:04,  2.40it/s]\u001b[A\n",
            "Iteration:  53% 176/330 [01:09<01:04,  2.40it/s]\u001b[A\n",
            "Iteration:  54% 177/330 [01:10<01:03,  2.39it/s]\u001b[A\n",
            "Iteration:  54% 178/330 [01:10<01:03,  2.39it/s]\u001b[A\n",
            "Iteration:  54% 179/330 [01:11<01:03,  2.39it/s]\u001b[A\n",
            "Iteration:  55% 180/330 [01:11<01:02,  2.39it/s]\u001b[A\n",
            "Iteration:  55% 181/330 [01:12<01:02,  2.39it/s]\u001b[A\n",
            "Iteration:  55% 182/330 [01:12<01:02,  2.38it/s]\u001b[A\n",
            "Iteration:  55% 183/330 [01:12<01:01,  2.38it/s]\u001b[A\n",
            "Iteration:  56% 184/330 [01:13<01:01,  2.37it/s]\u001b[A\n",
            "Iteration:  56% 185/330 [01:13<01:01,  2.37it/s]\u001b[A\n",
            "Iteration:  56% 186/330 [01:14<01:00,  2.38it/s]\u001b[A\n",
            "Iteration:  57% 187/330 [01:14<01:00,  2.37it/s]\u001b[A\n",
            "Iteration:  57% 188/330 [01:15<00:59,  2.37it/s]\u001b[A\n",
            "Iteration:  57% 189/330 [01:15<00:59,  2.37it/s]\u001b[A\n",
            "Iteration:  58% 190/330 [01:15<00:59,  2.36it/s]\u001b[A\n",
            "Iteration:  58% 191/330 [01:16<00:58,  2.36it/s]\u001b[A\n",
            "Iteration:  58% 192/330 [01:16<00:58,  2.36it/s]\u001b[A\n",
            "Iteration:  58% 193/330 [01:17<00:57,  2.36it/s]\u001b[A\n",
            "Iteration:  59% 194/330 [01:17<00:57,  2.36it/s]\u001b[A\n",
            "Iteration:  59% 195/330 [01:18<00:57,  2.36it/s]\u001b[A\n",
            "Iteration:  59% 196/330 [01:18<00:56,  2.36it/s]\u001b[A\n",
            "Iteration:  60% 197/330 [01:18<00:56,  2.35it/s]\u001b[A\n",
            "Iteration:  60% 198/330 [01:19<00:56,  2.35it/s]\u001b[A\n",
            "Iteration:  60% 199/330 [01:19<00:55,  2.35it/s]\u001b[A\n",
            "Iteration:  61% 200/330 [01:20<00:55,  2.35it/s]\u001b[A\n",
            "Iteration:  61% 201/330 [01:20<00:54,  2.35it/s]\u001b[A\n",
            "Iteration:  61% 202/330 [01:21<00:54,  2.35it/s]\u001b[A\n",
            "Iteration:  62% 203/330 [01:21<00:54,  2.35it/s]\u001b[A\n",
            "Iteration:  62% 204/330 [01:21<00:53,  2.34it/s]\u001b[A\n",
            "Iteration:  62% 205/330 [01:22<00:53,  2.34it/s]\u001b[A\n",
            "Iteration:  62% 206/330 [01:22<00:52,  2.34it/s]\u001b[A\n",
            "Iteration:  63% 207/330 [01:23<00:52,  2.34it/s]\u001b[A\n",
            "Iteration:  63% 208/330 [01:23<00:52,  2.34it/s]\u001b[A\n",
            "Iteration:  63% 209/330 [01:23<00:51,  2.34it/s]\u001b[A\n",
            "Iteration:  64% 210/330 [01:24<00:51,  2.34it/s]\u001b[A\n",
            "Iteration:  64% 211/330 [01:24<00:50,  2.34it/s]\u001b[A\n",
            "Iteration:  64% 212/330 [01:25<00:50,  2.34it/s]\u001b[A\n",
            "Iteration:  65% 213/330 [01:25<00:49,  2.35it/s]\u001b[A\n",
            "Iteration:  65% 214/330 [01:26<00:49,  2.34it/s]\u001b[A\n",
            "Iteration:  65% 215/330 [01:26<00:49,  2.34it/s]\u001b[A\n",
            "Iteration:  65% 216/330 [01:26<00:48,  2.34it/s]\u001b[A\n",
            "Iteration:  66% 217/330 [01:27<00:48,  2.34it/s]\u001b[A\n",
            "Iteration:  66% 218/330 [01:27<00:47,  2.35it/s]\u001b[A\n",
            "Iteration:  66% 219/330 [01:28<00:47,  2.35it/s]\u001b[A\n",
            "Iteration:  67% 220/330 [01:28<00:46,  2.35it/s]\u001b[A\n",
            "Iteration:  67% 221/330 [01:29<00:46,  2.35it/s]\u001b[A\n",
            "Iteration:  67% 222/330 [01:29<00:45,  2.36it/s]\u001b[A\n",
            "Iteration:  68% 223/330 [01:29<00:45,  2.35it/s]\u001b[A\n",
            "Iteration:  68% 224/330 [01:30<00:45,  2.35it/s]\u001b[A\n",
            "Iteration:  68% 225/330 [01:30<00:44,  2.35it/s]\u001b[A\n",
            "Iteration:  68% 226/330 [01:31<00:44,  2.36it/s]\u001b[A\n",
            "Iteration:  69% 227/330 [01:31<00:43,  2.36it/s]\u001b[A\n",
            "Iteration:  69% 228/330 [01:32<00:43,  2.36it/s]\u001b[A\n",
            "Iteration:  69% 229/330 [01:32<00:42,  2.37it/s]\u001b[A\n",
            "Iteration:  70% 230/330 [01:32<00:42,  2.37it/s]\u001b[A\n",
            "Iteration:  70% 231/330 [01:33<00:41,  2.37it/s]\u001b[A\n",
            "Iteration:  70% 232/330 [01:33<00:41,  2.37it/s]\u001b[A\n",
            "Iteration:  71% 233/330 [01:34<00:40,  2.37it/s]\u001b[A\n",
            "Iteration:  71% 234/330 [01:34<00:40,  2.38it/s]\u001b[A\n",
            "Iteration:  71% 235/330 [01:35<00:39,  2.38it/s]\u001b[A\n",
            "Iteration:  72% 236/330 [01:35<00:39,  2.38it/s]\u001b[A\n",
            "Iteration:  72% 237/330 [01:35<00:38,  2.39it/s]\u001b[A\n",
            "Iteration:  72% 238/330 [01:36<00:38,  2.39it/s]\u001b[A\n",
            "Iteration:  72% 239/330 [01:36<00:38,  2.39it/s]\u001b[A\n",
            "Iteration:  73% 240/330 [01:37<00:37,  2.39it/s]\u001b[A\n",
            "Iteration:  73% 241/330 [01:37<00:37,  2.39it/s]\u001b[A\n",
            "Iteration:  73% 242/330 [01:37<00:36,  2.40it/s]\u001b[A\n",
            "Iteration:  74% 243/330 [01:38<00:36,  2.40it/s]\u001b[A\n",
            "Iteration:  74% 244/330 [01:38<00:35,  2.40it/s]\u001b[A\n",
            "Iteration:  74% 245/330 [01:39<00:35,  2.40it/s]\u001b[A\n",
            "Iteration:  75% 246/330 [01:39<00:34,  2.40it/s]\u001b[A\n",
            "Iteration:  75% 247/330 [01:40<00:34,  2.41it/s]\u001b[A\n",
            "Iteration:  75% 248/330 [01:40<00:34,  2.41it/s]\u001b[A\n",
            "Iteration:  75% 249/330 [01:40<00:33,  2.41it/s]\u001b[A\n",
            "Iteration:  76% 250/330 [01:41<00:33,  2.41it/s]\u001b[A\n",
            "Iteration:  76% 251/330 [01:41<00:32,  2.40it/s]\u001b[A\n",
            "Iteration:  76% 252/330 [01:42<00:32,  2.41it/s]\u001b[A\n",
            "Iteration:  77% 253/330 [01:42<00:31,  2.41it/s]\u001b[A\n",
            "Iteration:  77% 254/330 [01:42<00:31,  2.42it/s]\u001b[A\n",
            "Iteration:  77% 255/330 [01:43<00:31,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 256/330 [01:43<00:30,  2.41it/s]\u001b[A\n",
            "Iteration:  78% 257/330 [01:44<00:30,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 258/330 [01:44<00:29,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 259/330 [01:44<00:29,  2.42it/s]\u001b[A\n",
            "Iteration:  79% 260/330 [01:45<00:28,  2.43it/s]\u001b[A\n",
            "Iteration:  79% 261/330 [01:45<00:28,  2.42it/s]\u001b[A\n",
            "Iteration:  79% 262/330 [01:46<00:28,  2.43it/s]\u001b[A\n",
            "Iteration:  80% 263/330 [01:46<00:27,  2.43it/s]\u001b[A\n",
            "Iteration:  80% 264/330 [01:47<00:27,  2.42it/s]\u001b[A\n",
            "Iteration:  80% 265/330 [01:47<00:26,  2.43it/s]\u001b[A\n",
            "Iteration:  81% 266/330 [01:47<00:26,  2.43it/s]\u001b[A\n",
            "Iteration:  81% 267/330 [01:48<00:25,  2.44it/s]\u001b[A\n",
            "Iteration:  81% 268/330 [01:48<00:25,  2.44it/s]\u001b[A\n",
            "Iteration:  82% 269/330 [01:49<00:25,  2.43it/s]\u001b[A\n",
            "Iteration:  82% 270/330 [01:49<00:24,  2.43it/s]\u001b[A\n",
            "Iteration:  82% 271/330 [01:49<00:24,  2.43it/s]\u001b[A\n",
            "Iteration:  82% 272/330 [01:50<00:23,  2.43it/s]\u001b[A\n",
            "Iteration:  83% 273/330 [01:50<00:23,  2.43it/s]\u001b[A\n",
            "Iteration:  83% 274/330 [01:51<00:22,  2.44it/s]\u001b[A\n",
            "Iteration:  83% 275/330 [01:51<00:22,  2.44it/s]\u001b[A\n",
            "Iteration:  84% 276/330 [01:51<00:22,  2.43it/s]\u001b[A\n",
            "Iteration:  84% 277/330 [01:52<00:21,  2.44it/s]\u001b[A\n",
            "Iteration:  84% 278/330 [01:52<00:21,  2.44it/s]\u001b[A\n",
            "Iteration:  85% 279/330 [01:53<00:20,  2.44it/s]\u001b[A\n",
            "Iteration:  85% 280/330 [01:53<00:20,  2.44it/s]\u001b[A\n",
            "Iteration:  85% 281/330 [01:54<00:20,  2.44it/s]\u001b[A\n",
            "Iteration:  85% 282/330 [01:54<00:19,  2.44it/s]\u001b[A\n",
            "Iteration:  86% 283/330 [01:54<00:19,  2.44it/s]\u001b[A\n",
            "Iteration:  86% 284/330 [01:55<00:18,  2.45it/s]\u001b[A\n",
            "Iteration:  86% 285/330 [01:55<00:18,  2.44it/s]\u001b[A\n",
            "Iteration:  87% 286/330 [01:56<00:18,  2.44it/s]\u001b[A\n",
            "Iteration:  87% 287/330 [01:56<00:17,  2.44it/s]\u001b[A\n",
            "Iteration:  87% 288/330 [01:56<00:17,  2.44it/s]\u001b[A\n",
            "Iteration:  88% 289/330 [01:57<00:16,  2.44it/s]\u001b[A\n",
            "Iteration:  88% 290/330 [01:57<00:16,  2.44it/s]\u001b[A\n",
            "Iteration:  88% 291/330 [01:58<00:15,  2.44it/s]\u001b[A\n",
            "Iteration:  88% 292/330 [01:58<00:15,  2.44it/s]\u001b[A\n",
            "Iteration:  89% 293/330 [01:58<00:15,  2.45it/s]\u001b[A\n",
            "Iteration:  89% 294/330 [01:59<00:14,  2.45it/s]\u001b[A\n",
            "Iteration:  89% 295/330 [01:59<00:14,  2.45it/s]\u001b[A\n",
            "Iteration:  90% 296/330 [02:00<00:13,  2.45it/s]\u001b[A\n",
            "Iteration:  90% 297/330 [02:00<00:13,  2.44it/s]\u001b[A\n",
            "Iteration:  90% 298/330 [02:00<00:13,  2.44it/s]\u001b[A\n",
            "Iteration:  91% 299/330 [02:01<00:12,  2.45it/s]\u001b[A\n",
            "Iteration:  91% 300/330 [02:01<00:12,  2.44it/s]\u001b[A\n",
            "Iteration:  91% 301/330 [02:02<00:11,  2.45it/s]\u001b[A\n",
            "Iteration:  92% 302/330 [02:02<00:11,  2.46it/s]\u001b[A\n",
            "Iteration:  92% 303/330 [02:03<00:11,  2.45it/s]\u001b[A\n",
            "Iteration:  92% 304/330 [02:03<00:10,  2.45it/s]\u001b[A\n",
            "Iteration:  92% 305/330 [02:03<00:10,  2.45it/s]\u001b[A\n",
            "Iteration:  93% 306/330 [02:04<00:09,  2.44it/s]\u001b[A\n",
            "Iteration:  93% 307/330 [02:04<00:09,  2.45it/s]\u001b[A\n",
            "Iteration:  93% 308/330 [02:05<00:08,  2.45it/s]\u001b[A\n",
            "Iteration:  94% 309/330 [02:05<00:08,  2.45it/s]\u001b[A\n",
            "Iteration:  94% 310/330 [02:05<00:08,  2.45it/s]\u001b[A\n",
            "Iteration:  94% 311/330 [02:06<00:07,  2.45it/s]\u001b[A\n",
            "Iteration:  95% 312/330 [02:06<00:07,  2.45it/s]\u001b[A\n",
            "Iteration:  95% 313/330 [02:07<00:06,  2.45it/s]\u001b[A\n",
            "Iteration:  95% 314/330 [02:07<00:06,  2.45it/s]\u001b[A\n",
            "Iteration:  95% 315/330 [02:07<00:06,  2.45it/s]\u001b[A\n",
            "Iteration:  96% 316/330 [02:08<00:05,  2.45it/s]\u001b[A\n",
            "Iteration:  96% 317/330 [02:08<00:05,  2.45it/s]\u001b[A\n",
            "Iteration:  96% 318/330 [02:09<00:04,  2.45it/s]\u001b[A\n",
            "Iteration:  97% 319/330 [02:09<00:04,  2.44it/s]\u001b[A\n",
            "Iteration:  97% 320/330 [02:09<00:04,  2.45it/s]\u001b[A\n",
            "Iteration:  97% 321/330 [02:10<00:03,  2.45it/s]\u001b[A\n",
            "Iteration:  98% 322/330 [02:10<00:03,  2.44it/s]\u001b[A\n",
            "Iteration:  98% 323/330 [02:11<00:02,  2.44it/s]\u001b[A\n",
            "Iteration:  98% 324/330 [02:11<00:02,  2.44it/s]\u001b[A\n",
            "Iteration:  98% 325/330 [02:12<00:02,  2.45it/s]\u001b[A\n",
            "Iteration:  99% 326/330 [02:12<00:01,  2.44it/s]\u001b[A\n",
            "Iteration:  99% 327/330 [02:12<00:01,  2.45it/s]\u001b[A\n",
            "Iteration:  99% 328/330 [02:13<00:00,  2.45it/s]\u001b[A\n",
            "Iteration: 100% 329/330 [02:13<00:00,  2.45it/s]\u001b[A\n",
            "Iteration: 100% 330/330 [02:14<00:00,  2.46it/s]\n",
            "Epoch:  50% 1/2 [02:14<02:14, 134.05s/it]\n",
            "Iteration:   0% 0/330 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/330 [00:00<02:14,  2.44it/s]\u001b[A\n",
            "Iteration:   1% 2/330 [00:00<02:13,  2.45it/s]\u001b[A\n",
            "Iteration:   1% 3/330 [00:01<02:13,  2.45it/s]\u001b[A\n",
            "Iteration:   1% 4/330 [00:01<02:13,  2.45it/s]\u001b[A\n",
            "Iteration:   2% 5/330 [00:02<02:12,  2.45it/s]\u001b[A\n",
            "Iteration:   2% 6/330 [00:02<02:12,  2.44it/s]\u001b[A\n",
            "Iteration:   2% 7/330 [00:02<02:12,  2.44it/s]\u001b[A\n",
            "Iteration:   2% 8/330 [00:03<02:12,  2.44it/s]\u001b[A\n",
            "Iteration:   3% 9/330 [00:03<02:11,  2.43it/s]\u001b[A\n",
            "Iteration:   3% 10/330 [00:04<02:11,  2.44it/s]\u001b[A\n",
            "Iteration:   3% 11/330 [00:04<02:11,  2.43it/s]\u001b[A\n",
            "Iteration:   4% 12/330 [00:04<02:10,  2.43it/s]\u001b[A\n",
            "Iteration:   4% 13/330 [00:05<02:10,  2.44it/s]\u001b[A\n",
            "Iteration:   4% 14/330 [00:05<02:09,  2.43it/s]\u001b[A\n",
            "Iteration:   5% 15/330 [00:06<02:09,  2.44it/s]\u001b[A\n",
            "Iteration:   5% 16/330 [00:06<02:08,  2.44it/s]\u001b[A\n",
            "Iteration:   5% 17/330 [00:06<02:08,  2.43it/s]\u001b[A\n",
            "Iteration:   5% 18/330 [00:07<02:08,  2.43it/s]\u001b[A\n",
            "Iteration:   6% 19/330 [00:07<02:07,  2.43it/s]\u001b[A\n",
            "Iteration:   6% 20/330 [00:08<02:07,  2.43it/s]\u001b[A\n",
            "Iteration:   6% 21/330 [00:08<02:06,  2.43it/s]\u001b[A\n",
            "Iteration:   7% 22/330 [00:09<02:06,  2.43it/s]\u001b[A\n",
            "Iteration:   7% 23/330 [00:09<02:06,  2.43it/s]\u001b[A\n",
            "Iteration:   7% 24/330 [00:09<02:05,  2.43it/s]\u001b[A\n",
            "Iteration:   8% 25/330 [00:10<02:05,  2.43it/s]\u001b[A\n",
            "Iteration:   8% 26/330 [00:10<02:05,  2.43it/s]\u001b[A\n",
            "Iteration:   8% 27/330 [00:11<02:04,  2.43it/s]\u001b[A\n",
            "Iteration:   8% 28/330 [00:11<02:04,  2.43it/s]\u001b[A\n",
            "Iteration:   9% 29/330 [00:11<02:04,  2.43it/s]\u001b[A\n",
            "Iteration:   9% 30/330 [00:12<02:03,  2.42it/s]\u001b[A\n",
            "Iteration:   9% 31/330 [00:12<02:03,  2.43it/s]\u001b[A\n",
            "Iteration:  10% 32/330 [00:13<02:02,  2.43it/s]\u001b[A\n",
            "Iteration:  10% 33/330 [00:13<02:02,  2.43it/s]\u001b[A\n",
            "Iteration:  10% 34/330 [00:13<02:02,  2.42it/s]\u001b[A\n",
            "Iteration:  11% 35/330 [00:14<02:01,  2.42it/s]\u001b[A\n",
            "Iteration:  11% 36/330 [00:14<02:01,  2.42it/s]\u001b[A\n",
            "Iteration:  11% 37/330 [00:15<02:00,  2.43it/s]\u001b[A\n",
            "Iteration:  12% 38/330 [00:15<02:00,  2.42it/s]\u001b[A\n",
            "Iteration:  12% 39/330 [00:16<02:00,  2.42it/s]\u001b[A\n",
            "Iteration:  12% 40/330 [00:16<01:59,  2.42it/s]\u001b[A\n",
            "Iteration:  12% 41/330 [00:16<01:59,  2.42it/s]\u001b[A\n",
            "Iteration:  13% 42/330 [00:17<01:58,  2.43it/s]\u001b[A\n",
            "Iteration:  13% 43/330 [00:17<01:58,  2.42it/s]\u001b[A\n",
            "Iteration:  13% 44/330 [00:18<01:58,  2.40it/s]\u001b[A\n",
            "Iteration:  14% 45/330 [00:18<01:58,  2.41it/s]\u001b[A\n",
            "Iteration:  14% 46/330 [00:18<01:57,  2.42it/s]\u001b[A\n",
            "Iteration:  14% 47/330 [00:19<01:57,  2.41it/s]\u001b[A\n",
            "Iteration:  15% 48/330 [00:19<01:56,  2.41it/s]\u001b[A\n",
            "Iteration:  15% 49/330 [00:20<01:56,  2.41it/s]\u001b[A\n",
            "Iteration:  15% 50/330 [00:20<01:56,  2.41it/s]\u001b[A\n",
            "Iteration:  15% 51/330 [00:21<01:55,  2.41it/s]\u001b[A\n",
            "Iteration:  16% 52/330 [00:21<01:55,  2.41it/s]\u001b[A\n",
            "Iteration:  16% 53/330 [00:21<01:54,  2.41it/s]\u001b[A\n",
            "Iteration:  16% 54/330 [00:22<01:54,  2.42it/s]\u001b[A\n",
            "Iteration:  17% 55/330 [00:22<01:53,  2.41it/s]\u001b[A\n",
            "Iteration:  17% 56/330 [00:23<01:53,  2.41it/s]\u001b[A\n",
            "Iteration:  17% 57/330 [00:23<01:53,  2.41it/s]\u001b[A\n",
            "Iteration:  18% 58/330 [00:23<01:52,  2.41it/s]\u001b[A\n",
            "Iteration:  18% 59/330 [00:24<01:52,  2.41it/s]\u001b[A\n",
            "Iteration:  18% 60/330 [00:24<01:51,  2.41it/s]\u001b[A\n",
            "Iteration:  18% 61/330 [00:25<01:51,  2.41it/s]\u001b[A\n",
            "Iteration:  19% 62/330 [00:25<01:51,  2.41it/s]\u001b[A\n",
            "Iteration:  19% 63/330 [00:25<01:50,  2.41it/s]\u001b[A\n",
            "Iteration:  19% 64/330 [00:26<01:50,  2.40it/s]\u001b[A\n",
            "Iteration:  20% 65/330 [00:26<01:50,  2.41it/s]\u001b[A\n",
            "Iteration:  20% 66/330 [00:27<01:49,  2.40it/s]\u001b[A\n",
            "Iteration:  20% 67/330 [00:27<01:49,  2.41it/s]\u001b[A\n",
            "Iteration:  21% 68/330 [00:28<01:48,  2.41it/s]\u001b[A\n",
            "Iteration:  21% 69/330 [00:28<01:48,  2.41it/s]\u001b[A\n",
            "Iteration:  21% 70/330 [00:28<01:48,  2.40it/s]\u001b[A\n",
            "Iteration:  22% 71/330 [00:29<01:47,  2.40it/s]\u001b[A\n",
            "Iteration:  22% 72/330 [00:29<01:47,  2.40it/s]\u001b[A\n",
            "Iteration:  22% 73/330 [00:30<01:47,  2.40it/s]\u001b[A\n",
            "Iteration:  22% 74/330 [00:30<01:46,  2.40it/s]\u001b[A\n",
            "Iteration:  23% 75/330 [00:30<01:46,  2.40it/s]\u001b[A\n",
            "Iteration:  23% 76/330 [00:31<01:45,  2.40it/s]\u001b[A\n",
            "Iteration:  23% 77/330 [00:31<01:45,  2.40it/s]\u001b[A\n",
            "Iteration:  24% 78/330 [00:32<01:44,  2.41it/s]\u001b[A\n",
            "Iteration:  24% 79/330 [00:32<01:44,  2.41it/s]\u001b[A\n",
            "Iteration:  24% 80/330 [00:33<01:43,  2.41it/s]\u001b[A\n",
            "Iteration:  25% 81/330 [00:33<01:43,  2.40it/s]\u001b[A\n",
            "Iteration:  25% 82/330 [00:33<01:43,  2.41it/s]\u001b[A\n",
            "Iteration:  25% 83/330 [00:34<01:42,  2.41it/s]\u001b[A\n",
            "Iteration:  25% 84/330 [00:34<01:42,  2.40it/s]\u001b[A\n",
            "Iteration:  26% 85/330 [00:35<01:41,  2.40it/s]\u001b[A\n",
            "Iteration:  26% 86/330 [00:35<01:41,  2.40it/s]\u001b[A\n",
            "Iteration:  26% 87/330 [00:35<01:40,  2.41it/s]\u001b[A\n",
            "Iteration:  27% 88/330 [00:36<01:40,  2.40it/s]\u001b[A\n",
            "Iteration:  27% 89/330 [00:36<01:40,  2.40it/s]\u001b[A\n",
            "Iteration:  27% 90/330 [00:37<01:39,  2.40it/s]\u001b[A\n",
            "Iteration:  28% 91/330 [00:37<01:39,  2.40it/s]\u001b[A\n",
            "Iteration:  28% 92/330 [00:38<01:39,  2.40it/s]\u001b[A\n",
            "Iteration:  28% 93/330 [00:38<01:38,  2.41it/s]\u001b[A\n",
            "Iteration:  28% 94/330 [00:38<01:38,  2.40it/s]\u001b[A\n",
            "Iteration:  29% 95/330 [00:39<01:37,  2.40it/s]\u001b[A\n",
            "Iteration:  29% 96/330 [00:39<01:37,  2.39it/s]\u001b[A\n",
            "Iteration:  29% 97/330 [00:40<01:37,  2.40it/s]\u001b[A\n",
            "Iteration:  30% 98/330 [00:40<01:36,  2.40it/s]\u001b[A\n",
            "Iteration:  30% 99/330 [00:40<01:36,  2.39it/s]\u001b[A\n",
            "Iteration:  30% 100/330 [00:41<01:36,  2.40it/s]\u001b[A\n",
            "Iteration:  31% 101/330 [00:41<01:35,  2.39it/s]\u001b[A\n",
            "Iteration:  31% 102/330 [00:42<01:34,  2.40it/s]\u001b[A\n",
            "Iteration:  31% 103/330 [00:42<01:34,  2.40it/s]\u001b[A\n",
            "Iteration:  32% 104/330 [00:43<01:34,  2.40it/s]\u001b[A\n",
            "Iteration:  32% 105/330 [00:43<01:33,  2.40it/s]\u001b[A\n",
            "Iteration:  32% 106/330 [00:43<01:33,  2.39it/s]\u001b[A\n",
            "Iteration:  32% 107/330 [00:44<01:32,  2.40it/s]\u001b[A\n",
            "Iteration:  33% 108/330 [00:44<01:32,  2.40it/s]\u001b[A\n",
            "Iteration:  33% 109/330 [00:45<01:31,  2.41it/s]\u001b[A\n",
            "Iteration:  33% 110/330 [00:45<01:31,  2.40it/s]\u001b[A\n",
            "Iteration:  34% 111/330 [00:45<01:30,  2.41it/s]\u001b[A\n",
            "Iteration:  34% 112/330 [00:46<01:30,  2.41it/s]\u001b[A\n",
            "Iteration:  34% 113/330 [00:46<01:30,  2.41it/s]\u001b[A\n",
            "Iteration:  35% 114/330 [00:47<01:29,  2.40it/s]\u001b[A\n",
            "Iteration:  35% 115/330 [00:47<01:29,  2.41it/s]\u001b[A\n",
            "Iteration:  35% 116/330 [00:48<01:28,  2.41it/s]\u001b[A\n",
            "Iteration:  35% 117/330 [00:48<01:28,  2.41it/s]\u001b[A\n",
            "Iteration:  36% 118/330 [00:48<01:28,  2.41it/s]\u001b[A\n",
            "Iteration:  36% 119/330 [00:49<01:27,  2.41it/s]\u001b[A\n",
            "Iteration:  36% 120/330 [00:49<01:27,  2.41it/s]\u001b[A\n",
            "Iteration:  37% 121/330 [00:50<01:26,  2.41it/s]\u001b[A\n",
            "Iteration:  37% 122/330 [00:50<01:26,  2.41it/s]\u001b[A\n",
            "Iteration:  37% 123/330 [00:50<01:25,  2.41it/s]\u001b[A\n",
            "Iteration:  38% 124/330 [00:51<01:25,  2.41it/s]\u001b[A\n",
            "Iteration:  38% 125/330 [00:51<01:25,  2.41it/s]\u001b[A\n",
            "Iteration:  38% 126/330 [00:52<01:24,  2.41it/s]\u001b[A\n",
            "Iteration:  38% 127/330 [00:52<01:24,  2.41it/s]\u001b[A\n",
            "Iteration:  39% 128/330 [00:53<01:23,  2.41it/s]\u001b[A\n",
            "Iteration:  39% 129/330 [00:53<01:23,  2.41it/s]\u001b[A\n",
            "Iteration:  39% 130/330 [00:53<01:22,  2.41it/s]\u001b[A\n",
            "Iteration:  40% 131/330 [00:54<01:22,  2.41it/s]\u001b[A\n",
            "Iteration:  40% 132/330 [00:54<01:21,  2.42it/s]\u001b[A\n",
            "Iteration:  40% 133/330 [00:55<01:21,  2.42it/s]\u001b[A\n",
            "Iteration:  41% 134/330 [00:55<01:20,  2.42it/s]\u001b[A\n",
            "Iteration:  41% 135/330 [00:55<01:20,  2.42it/s]\u001b[A\n",
            "Iteration:  41% 136/330 [00:56<01:20,  2.42it/s]\u001b[A\n",
            "Iteration:  42% 137/330 [00:56<01:19,  2.42it/s]\u001b[A\n",
            "Iteration:  42% 138/330 [00:57<01:19,  2.42it/s]\u001b[A\n",
            "Iteration:  42% 139/330 [00:57<01:19,  2.41it/s]\u001b[A\n",
            "Iteration:  42% 140/330 [00:57<01:18,  2.43it/s]\u001b[A\n",
            "Iteration:  43% 141/330 [00:58<01:18,  2.42it/s]\u001b[A\n",
            "Iteration:  43% 142/330 [00:58<01:17,  2.42it/s]\u001b[A\n",
            "Iteration:  43% 143/330 [00:59<01:17,  2.42it/s]\u001b[A\n",
            "Iteration:  44% 144/330 [00:59<01:16,  2.42it/s]\u001b[A\n",
            "Iteration:  44% 145/330 [01:00<01:16,  2.42it/s]\u001b[A\n",
            "Iteration:  44% 146/330 [01:00<01:16,  2.42it/s]\u001b[A\n",
            "Iteration:  45% 147/330 [01:00<01:15,  2.42it/s]\u001b[A\n",
            "Iteration:  45% 148/330 [01:01<01:15,  2.42it/s]\u001b[A\n",
            "Iteration:  45% 149/330 [01:01<01:14,  2.42it/s]\u001b[A\n",
            "Iteration:  45% 150/330 [01:02<01:14,  2.42it/s]\u001b[A\n",
            "Iteration:  46% 151/330 [01:02<01:13,  2.42it/s]\u001b[A\n",
            "Iteration:  46% 152/330 [01:02<01:13,  2.43it/s]\u001b[A\n",
            "Iteration:  46% 153/330 [01:03<01:12,  2.43it/s]\u001b[A\n",
            "Iteration:  47% 154/330 [01:03<01:12,  2.42it/s]\u001b[A\n",
            "Iteration:  47% 155/330 [01:04<01:12,  2.42it/s]\u001b[A\n",
            "Iteration:  47% 156/330 [01:04<01:12,  2.41it/s]\u001b[A\n",
            "Iteration:  48% 157/330 [01:05<01:11,  2.42it/s]\u001b[A\n",
            "Iteration:  48% 158/330 [01:05<01:10,  2.43it/s]\u001b[A\n",
            "Iteration:  48% 159/330 [01:05<01:10,  2.42it/s]\u001b[A\n",
            "Iteration:  48% 160/330 [01:06<01:10,  2.42it/s]\u001b[A\n",
            "Iteration:  49% 161/330 [01:06<01:09,  2.42it/s]\u001b[A\n",
            "Iteration:  49% 162/330 [01:07<01:09,  2.41it/s]\u001b[A\n",
            "Iteration:  49% 163/330 [01:07<01:08,  2.42it/s]\u001b[A\n",
            "Iteration:  50% 164/330 [01:07<01:08,  2.42it/s]\u001b[A\n",
            "Iteration:  50% 165/330 [01:08<01:08,  2.42it/s]\u001b[A\n",
            "Iteration:  50% 166/330 [01:08<01:07,  2.43it/s]\u001b[A\n",
            "Iteration:  51% 167/330 [01:09<01:07,  2.42it/s]\u001b[A\n",
            "Iteration:  51% 168/330 [01:09<01:06,  2.42it/s]\u001b[A\n",
            "Iteration:  51% 169/330 [01:09<01:06,  2.42it/s]\u001b[A{'loss': 3.4966158623695374, 'learning_rate': 1.2121212121212122e-05, 'epoch': 1.5151515151515151, 'step': 500}\n",
            "\n",
            "Iteration:  52% 170/330 [01:10<01:06,  2.41it/s]\u001b[A\n",
            "Iteration:  52% 171/330 [01:10<01:05,  2.42it/s]\u001b[A\n",
            "Iteration:  52% 172/330 [01:11<01:05,  2.42it/s]\u001b[A\n",
            "Iteration:  52% 173/330 [01:11<01:04,  2.42it/s]\u001b[A\n",
            "Iteration:  53% 174/330 [01:12<01:04,  2.43it/s]\u001b[A\n",
            "Iteration:  53% 175/330 [01:12<01:04,  2.42it/s]\u001b[A\n",
            "Iteration:  53% 176/330 [01:12<01:03,  2.43it/s]\u001b[A\n",
            "Iteration:  54% 177/330 [01:13<01:03,  2.42it/s]\u001b[A\n",
            "Iteration:  54% 178/330 [01:13<01:02,  2.42it/s]\u001b[A\n",
            "Iteration:  54% 179/330 [01:14<01:02,  2.43it/s]\u001b[A\n",
            "Iteration:  55% 180/330 [01:14<01:01,  2.43it/s]\u001b[A\n",
            "Iteration:  55% 181/330 [01:14<01:01,  2.42it/s]\u001b[A\n",
            "Iteration:  55% 182/330 [01:15<01:01,  2.42it/s]\u001b[A\n",
            "Iteration:  55% 183/330 [01:15<01:00,  2.43it/s]\u001b[A\n",
            "Iteration:  56% 184/330 [01:16<01:00,  2.43it/s]\u001b[A\n",
            "Iteration:  56% 185/330 [01:16<00:59,  2.42it/s]\u001b[A\n",
            "Iteration:  56% 186/330 [01:16<00:59,  2.43it/s]\u001b[A\n",
            "Iteration:  57% 187/330 [01:17<00:59,  2.42it/s]\u001b[A\n",
            "Iteration:  57% 188/330 [01:17<00:58,  2.42it/s]\u001b[A\n",
            "Iteration:  57% 189/330 [01:18<00:58,  2.43it/s]\u001b[A\n",
            "Iteration:  58% 190/330 [01:18<00:57,  2.43it/s]\u001b[A\n",
            "Iteration:  58% 191/330 [01:19<00:57,  2.42it/s]\u001b[A\n",
            "Iteration:  58% 192/330 [01:19<00:57,  2.41it/s]\u001b[A\n",
            "Iteration:  58% 193/330 [01:19<00:56,  2.42it/s]\u001b[A\n",
            "Iteration:  59% 194/330 [01:20<00:56,  2.42it/s]\u001b[A\n",
            "Iteration:  59% 195/330 [01:20<00:55,  2.42it/s]\u001b[A\n",
            "Iteration:  59% 196/330 [01:21<00:55,  2.42it/s]\u001b[A\n",
            "Iteration:  60% 197/330 [01:21<00:54,  2.42it/s]\u001b[A\n",
            "Iteration:  60% 198/330 [01:21<00:54,  2.42it/s]\u001b[A\n",
            "Iteration:  60% 199/330 [01:22<00:54,  2.42it/s]\u001b[A\n",
            "Iteration:  61% 200/330 [01:22<00:53,  2.42it/s]\u001b[A\n",
            "Iteration:  61% 201/330 [01:23<00:53,  2.42it/s]\u001b[A\n",
            "Iteration:  61% 202/330 [01:23<00:52,  2.42it/s]\u001b[A\n",
            "Iteration:  62% 203/330 [01:24<00:52,  2.42it/s]\u001b[A\n",
            "Iteration:  62% 204/330 [01:24<00:52,  2.42it/s]\u001b[A\n",
            "Iteration:  62% 205/330 [01:24<00:51,  2.42it/s]\u001b[A\n",
            "Iteration:  62% 206/330 [01:25<00:51,  2.42it/s]\u001b[A\n",
            "Iteration:  63% 207/330 [01:25<00:50,  2.42it/s]\u001b[A\n",
            "Iteration:  63% 208/330 [01:26<00:50,  2.42it/s]\u001b[A\n",
            "Iteration:  63% 209/330 [01:26<00:49,  2.42it/s]\u001b[A\n",
            "Iteration:  64% 210/330 [01:26<00:49,  2.42it/s]\u001b[A\n",
            "Iteration:  64% 211/330 [01:27<00:49,  2.42it/s]\u001b[A\n",
            "Iteration:  64% 212/330 [01:27<00:48,  2.42it/s]\u001b[A\n",
            "Iteration:  65% 213/330 [01:28<00:48,  2.42it/s]\u001b[A\n",
            "Iteration:  65% 214/330 [01:28<00:47,  2.42it/s]\u001b[A\n",
            "Iteration:  65% 215/330 [01:28<00:47,  2.42it/s]\u001b[A\n",
            "Iteration:  65% 216/330 [01:29<00:47,  2.42it/s]\u001b[A\n",
            "Iteration:  66% 217/330 [01:29<00:46,  2.43it/s]\u001b[A\n",
            "Iteration:  66% 218/330 [01:30<00:46,  2.42it/s]\u001b[A\n",
            "Iteration:  66% 219/330 [01:30<00:45,  2.42it/s]\u001b[A\n",
            "Iteration:  67% 220/330 [01:31<00:45,  2.42it/s]\u001b[A\n",
            "Iteration:  67% 221/330 [01:31<00:45,  2.42it/s]\u001b[A\n",
            "Iteration:  67% 222/330 [01:31<00:44,  2.42it/s]\u001b[A\n",
            "Iteration:  68% 223/330 [01:32<00:44,  2.42it/s]\u001b[A\n",
            "Iteration:  68% 224/330 [01:32<00:43,  2.42it/s]\u001b[A\n",
            "Iteration:  68% 225/330 [01:33<00:43,  2.42it/s]\u001b[A\n",
            "Iteration:  68% 226/330 [01:33<00:43,  2.41it/s]\u001b[A\n",
            "Iteration:  69% 227/330 [01:33<00:42,  2.42it/s]\u001b[A\n",
            "Iteration:  69% 228/330 [01:34<00:42,  2.42it/s]\u001b[A\n",
            "Iteration:  69% 229/330 [01:34<00:41,  2.42it/s]\u001b[A\n",
            "Iteration:  70% 230/330 [01:35<00:41,  2.43it/s]\u001b[A\n",
            "Iteration:  70% 231/330 [01:35<00:40,  2.42it/s]\u001b[A\n",
            "Iteration:  70% 232/330 [01:35<00:40,  2.42it/s]\u001b[A\n",
            "Iteration:  71% 233/330 [01:36<00:40,  2.42it/s]\u001b[A\n",
            "Iteration:  71% 234/330 [01:36<00:39,  2.42it/s]\u001b[A\n",
            "Iteration:  71% 235/330 [01:37<00:39,  2.42it/s]\u001b[A\n",
            "Iteration:  72% 236/330 [01:37<00:38,  2.42it/s]\u001b[A\n",
            "Iteration:  72% 237/330 [01:38<00:38,  2.42it/s]\u001b[A\n",
            "Iteration:  72% 238/330 [01:38<00:37,  2.43it/s]\u001b[A\n",
            "Iteration:  72% 239/330 [01:38<00:37,  2.42it/s]\u001b[A\n",
            "Iteration:  73% 240/330 [01:39<00:37,  2.41it/s]\u001b[A\n",
            "Iteration:  73% 241/330 [01:39<00:36,  2.41it/s]\u001b[A\n",
            "Iteration:  73% 242/330 [01:40<00:36,  2.42it/s]\u001b[A\n",
            "Iteration:  74% 243/330 [01:40<00:35,  2.42it/s]\u001b[A\n",
            "Iteration:  74% 244/330 [01:40<00:35,  2.42it/s]\u001b[A\n",
            "Iteration:  74% 245/330 [01:41<00:35,  2.42it/s]\u001b[A\n",
            "Iteration:  75% 246/330 [01:41<00:34,  2.41it/s]\u001b[A\n",
            "Iteration:  75% 247/330 [01:42<00:34,  2.41it/s]\u001b[A\n",
            "Iteration:  75% 248/330 [01:42<00:33,  2.42it/s]\u001b[A\n",
            "Iteration:  75% 249/330 [01:43<00:33,  2.41it/s]\u001b[A\n",
            "Iteration:  76% 250/330 [01:43<00:33,  2.41it/s]\u001b[A\n",
            "Iteration:  76% 251/330 [01:43<00:32,  2.42it/s]\u001b[A\n",
            "Iteration:  76% 252/330 [01:44<00:32,  2.41it/s]\u001b[A\n",
            "Iteration:  77% 253/330 [01:44<00:31,  2.41it/s]\u001b[A\n",
            "Iteration:  77% 254/330 [01:45<00:31,  2.41it/s]\u001b[A\n",
            "Iteration:  77% 255/330 [01:45<00:31,  2.41it/s]\u001b[A\n",
            "Iteration:  78% 256/330 [01:45<00:30,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 257/330 [01:46<00:30,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 258/330 [01:46<00:29,  2.42it/s]\u001b[A\n",
            "Iteration:  78% 259/330 [01:47<00:29,  2.42it/s]\u001b[A\n",
            "Iteration:  79% 260/330 [01:47<00:29,  2.41it/s]\u001b[A\n",
            "Iteration:  79% 261/330 [01:47<00:28,  2.41it/s]\u001b[A\n",
            "Iteration:  79% 262/330 [01:48<00:28,  2.41it/s]\u001b[A\n",
            "Iteration:  80% 263/330 [01:48<00:27,  2.42it/s]\u001b[A\n",
            "Iteration:  80% 264/330 [01:49<00:27,  2.41it/s]\u001b[A\n",
            "Iteration:  80% 265/330 [01:49<00:26,  2.41it/s]\u001b[A\n",
            "Iteration:  81% 266/330 [01:50<00:26,  2.41it/s]\u001b[A\n",
            "Iteration:  81% 267/330 [01:50<00:26,  2.41it/s]\u001b[A\n",
            "Iteration:  81% 268/330 [01:50<00:25,  2.41it/s]\u001b[A\n",
            "Iteration:  82% 269/330 [01:51<00:25,  2.41it/s]\u001b[A\n",
            "Iteration:  82% 270/330 [01:51<00:24,  2.41it/s]\u001b[A\n",
            "Iteration:  82% 271/330 [01:52<00:24,  2.41it/s]\u001b[A\n",
            "Iteration:  82% 272/330 [01:52<00:24,  2.41it/s]\u001b[A\n",
            "Iteration:  83% 273/330 [01:52<00:23,  2.41it/s]\u001b[A\n",
            "Iteration:  83% 274/330 [01:53<00:23,  2.42it/s]\u001b[A\n",
            "Iteration:  83% 275/330 [01:53<00:22,  2.41it/s]\u001b[A\n",
            "Iteration:  84% 276/330 [01:54<00:22,  2.41it/s]\u001b[A\n",
            "Iteration:  84% 277/330 [01:54<00:21,  2.41it/s]\u001b[A\n",
            "Iteration:  84% 278/330 [01:55<00:21,  2.41it/s]\u001b[A\n",
            "Iteration:  85% 279/330 [01:55<00:21,  2.41it/s]\u001b[A\n",
            "Iteration:  85% 280/330 [01:55<00:20,  2.41it/s]\u001b[A\n",
            "Iteration:  85% 281/330 [01:56<00:20,  2.42it/s]\u001b[A\n",
            "Iteration:  85% 282/330 [01:56<00:19,  2.41it/s]\u001b[A\n",
            "Iteration:  86% 283/330 [01:57<00:19,  2.41it/s]\u001b[A\n",
            "Iteration:  86% 284/330 [01:57<00:19,  2.41it/s]\u001b[A\n",
            "Iteration:  86% 285/330 [01:57<00:18,  2.41it/s]\u001b[A\n",
            "Iteration:  87% 286/330 [01:58<00:18,  2.41it/s]\u001b[A\n",
            "Iteration:  87% 287/330 [01:58<00:17,  2.41it/s]\u001b[A\n",
            "Iteration:  87% 288/330 [01:59<00:17,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 289/330 [01:59<00:17,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 290/330 [02:00<00:16,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 291/330 [02:00<00:16,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 292/330 [02:00<00:15,  2.40it/s]\u001b[A\n",
            "Iteration:  89% 293/330 [02:01<00:15,  2.40it/s]\u001b[A\n",
            "Iteration:  89% 294/330 [02:01<00:14,  2.40it/s]\u001b[A\n",
            "Iteration:  89% 295/330 [02:02<00:14,  2.41it/s]\u001b[A\n",
            "Iteration:  90% 296/330 [02:02<00:14,  2.40it/s]\u001b[A\n",
            "Iteration:  90% 297/330 [02:02<00:13,  2.40it/s]\u001b[A\n",
            "Iteration:  90% 298/330 [02:03<00:13,  2.41it/s]\u001b[A\n",
            "Iteration:  91% 299/330 [02:03<00:12,  2.41it/s]\u001b[A\n",
            "Iteration:  91% 300/330 [02:04<00:12,  2.40it/s]\u001b[A\n",
            "Iteration:  91% 301/330 [02:04<00:12,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 302/330 [02:05<00:11,  2.41it/s]\u001b[A\n",
            "Iteration:  92% 303/330 [02:05<00:11,  2.41it/s]\u001b[A\n",
            "Iteration:  92% 304/330 [02:05<00:10,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 305/330 [02:06<00:10,  2.40it/s]\u001b[A\n",
            "Iteration:  93% 306/330 [02:06<00:09,  2.40it/s]\u001b[A\n",
            "Iteration:  93% 307/330 [02:07<00:09,  2.41it/s]\u001b[A\n",
            "Iteration:  93% 308/330 [02:07<00:09,  2.40it/s]\u001b[A\n",
            "Iteration:  94% 309/330 [02:07<00:08,  2.40it/s]\u001b[A\n",
            "Iteration:  94% 310/330 [02:08<00:08,  2.41it/s]\u001b[A\n",
            "Iteration:  94% 311/330 [02:08<00:07,  2.40it/s]\u001b[A\n",
            "Iteration:  95% 312/330 [02:09<00:07,  2.41it/s]\u001b[A\n",
            "Iteration:  95% 313/330 [02:09<00:07,  2.41it/s]\u001b[A\n",
            "Iteration:  95% 314/330 [02:10<00:06,  2.41it/s]\u001b[A\n",
            "Iteration:  95% 315/330 [02:10<00:06,  2.41it/s]\u001b[A\n",
            "Iteration:  96% 316/330 [02:10<00:05,  2.41it/s]\u001b[A\n",
            "Iteration:  96% 317/330 [02:11<00:05,  2.41it/s]\u001b[A\n",
            "Iteration:  96% 318/330 [02:11<00:04,  2.41it/s]\u001b[A\n",
            "Iteration:  97% 319/330 [02:12<00:04,  2.40it/s]\u001b[A\n",
            "Iteration:  97% 320/330 [02:12<00:04,  2.41it/s]\u001b[A\n",
            "Iteration:  97% 321/330 [02:12<00:03,  2.40it/s]\u001b[A\n",
            "Iteration:  98% 322/330 [02:13<00:03,  2.41it/s]\u001b[A\n",
            "Iteration:  98% 323/330 [02:13<00:02,  2.41it/s]\u001b[A\n",
            "Iteration:  98% 324/330 [02:14<00:02,  2.41it/s]\u001b[A\n",
            "Iteration:  98% 325/330 [02:14<00:02,  2.41it/s]\u001b[A\n",
            "Iteration:  99% 326/330 [02:14<00:01,  2.40it/s]\u001b[A\n",
            "Iteration:  99% 327/330 [02:15<00:01,  2.41it/s]\u001b[A\n",
            "Iteration:  99% 328/330 [02:15<00:00,  2.40it/s]\u001b[A\n",
            "Iteration: 100% 329/330 [02:16<00:00,  2.41it/s]\u001b[A\n",
            "Iteration: 100% 330/330 [02:16<00:00,  2.41it/s]\n",
            "Epoch: 100% 2/2 [04:30<00:00, 135.35s/it]\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to output\n",
            "Configuration saved in output/config.json\n",
            "Model weights saved in output/pytorch_model.bin\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/trainer.py:1063: FutureWarning: This method is deprecated, use `Trainer.is_world_process_zero()` instead.\n",
            "  warnings.warn(\"This method is deprecated, use `Trainer.is_world_process_zero()` instead.\", FutureWarning)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dW3Myq_TXUEW",
        "colab_type": "text"
      },
      "source": [
        "Loading the tokenizer and model from output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lknh8RcicHd",
        "colab_type": "text"
      },
      "source": [
        "We have saved the tokenizer and model in the output directory using the run_language_modeling.py script. Now, we will load them using GPT2Tokenizer, GPT2LMHeadModel imported from transformers package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaXpzPq-IOnN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "3f2a8fcd-84a3-4631-fe15-d9c58a8e350b"
      },
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/output')\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/output')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "TensorFlow version 2.3.0 available.\n",
            "Model name '/content/output' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "Didn't find file /content/output/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/output/tokenizer.json. We won't load it.\n",
            "loading file /content/output/vocab.json\n",
            "loading file /content/output/merges.txt\n",
            "loading file None\n",
            "loading file /content/output/special_tokens_map.json\n",
            "loading file /content/output/tokenizer_config.json\n",
            "loading file None\n",
            "loading configuration file /content/output/config.json\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file /content/output/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/output.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MhORLgfi60y",
        "colab_type": "text"
      },
      "source": [
        "We have our model and tokenizer with us and it's time that we use them to generate some interesting texts similar to Shakespeare's plays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJN_4O4YXamO",
        "colab_type": "text"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr0nBJSPKDWO",
        "colab_type": "text"
      },
      "source": [
        "It is very important that we understand how do we generate the new outputs using the fine-tuned model so as to bring the best results.\n",
        "\n",
        "We will use many different approaches to generate text and find out which one works out to be the best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVn4mv_GKWhC",
        "colab_type": "text"
      },
      "source": [
        "## Greedy Search\n",
        "This is a very basic searching algorithm which selects the word with highest probability as its next word and doesn't use other words with lesser probability.\n",
        "The code for implementing greedy search with our model is given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDjk6SIvjobw",
        "colab_type": "text"
      },
      "source": [
        "We will first use the tokenizer to encode the prompt we want to give to the model to start off with generating the text.\n",
        "Then we'll use generate function to generate the new text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWLI8HpxkTjH",
        "colab_type": "text"
      },
      "source": [
        "Here, we have added [WP] for starting the prompt and endprompts which makes it easier for the model to generate text based on the example input sentence which is 'The King must leave the throne now .' in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz0yxhjCJEBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82d4c786-0494-486f-a6ef-1ed344b643b0"
      },
      "source": [
        "ids1 = tokenizer.encode('[ WP ] The King must leave the throne now . <endprompts>',\n",
        "                      return_tensors='pt')\n",
        "\n",
        "greedy_outputs = model.generate(ids1, max_length=300)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, greedy_output in enumerate(greedy_outputs):\n",
        "  print(\"\\n\"+\"===\"*10)\n",
        "  print(\"{}: {}\".format(i+1, tokenizer.decode(greedy_output, skip_special_tokens=False)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "==============================\n",
            "1: [ WP ] The King must leave the throne now. <endprompts>\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I will not.\n",
            "\n",
            "KING RICHARD II:\n",
            "I will not.\n",
            "\n",
            "GLOUCESTER:\n",
            "I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtgUI-w6L7Ml",
        "colab_type": "text"
      },
      "source": [
        "As you can see, it gives an output which has too much repition and clearly it is not able to generate good text for the play."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CFk3tH5MR8p",
        "colab_type": "text"
      },
      "source": [
        "So, next we try out beam search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rllW7EwPMV2O",
        "colab_type": "text"
      },
      "source": [
        "## Beam Search\n",
        "It is a search algorithm which considers the probabilities of consequent no (num_beams) of words not like greedy search which simply selects word with highest probability. It then multiplies these probabilities with the previous ones for each case. Then, it selects the sequence of words which had higher overall probability after multiplication.\n",
        "\n",
        "The code for implementing beam search with our model is given below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H41cbQ2iOG65",
        "colab_type": "text"
      },
      "source": [
        "We set num_beams > 1 and early_stopping=True so that generation is finished when all beam hypotheses reached the endprompts token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmANZqLXJGt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "outputId": "dfdba1d5-ed82-4684-ecc4-ec99cc88f411"
      },
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    ids1, \n",
        "    max_length=300, \n",
        "    num_beams=4, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[ WP ] The King must leave the throne now. <endprompts>\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "The king must leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai2FIQH1RCM2",
        "colab_type": "text"
      },
      "source": [
        "This shows that beam search alone is also not good enough and we will have to add some more parameters in generate function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfF7Q8-6RddF",
        "colab_type": "text"
      },
      "source": [
        "## Let's add Sampling\n",
        "Sampling means randomly picking the next word according to its conditional probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A7ULJkYlSf-",
        "colab_type": "text"
      },
      "source": [
        "We need to import tensorflow to help us set seed and induce random sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cDU-YE0RY3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d79Xm8lYlcIw",
        "colab_type": "text"
      },
      "source": [
        "The do_sample=True lets us produce sampling for the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUC74nGbOa_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "7d9d6307-24c3-4bbc-e93c-043e76802dd8"
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    ids1, \n",
        "    do_sample=True, \n",
        "    max_length=300\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[ WP ] The King must leave the throne now. <endprompts> KING HENRY VI\n",
            "\n",
            "GRENILIA:\n",
            "The king shall and do.\n",
            "\n",
            "LEONTES:\n",
            "Well may you think it, Henry. If he so desire, he shall bring some news.\n",
            "\n",
            "KING HENRY VI:\n",
            "I take it, good lady!\n",
            "For the moment I can't bear the thought, I stand still: for if the Duke of Clarence would,\n",
            "I know he is but too late. He is coming to hear me,\n",
            "and he shall answer, my lord, this matter should be brought to his mouth.\n",
            "\n",
            "GRENILIA:\n",
            "Is not the Duke of Clarence's word enough?\n",
            "\n",
            "KING HENRY VI:\n",
            "Not much.\n",
            "\n",
            "GRENILIA:\n",
            "Yet, when she speaks, speak, and I shall hear.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ay, for the day I must hear his answer. He will not speak.\n",
            "\n",
            "GRENILIA:\n",
            "The Duke of Clarence must answer you.\n",
            "\n",
            "KING HENRY VI:\n",
            "You will not?\n",
            "\n",
            "GRENILIA:\n",
            "Not to say he would. When I speak, I take no part in the proceedings.\n",
            "\n",
            "KING HENRY VI:\n",
            "When he shows time, the Duke of Clarence will hear you.\n",
            "\n",
            "GRENILIA:\n",
            "The Duke of Clarence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u9P1KqkRx3L",
        "colab_type": "text"
      },
      "source": [
        "As we can see it produce much better results than previous ones and the text is also starting to make some sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnMUEgxrS1MR",
        "colab_type": "text"
      },
      "source": [
        "## Top-K Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCEow0CpS2UX",
        "colab_type": "text"
      },
      "source": [
        "Let's try something new.\n",
        "Top-k sampling has recently become a popular alternative sampling procedure (Fan et al., 2018;\n",
        "Holtzman et al., 2018; Radford et al., 2019). Nucleus Sampling and top-k both sample from truncated Neural LM distributions, differing only in the strategy of where to truncate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1A3CC4JTwt4",
        "colab_type": "text"
      },
      "source": [
        "In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOKCxnTcT9QZ",
        "colab_type": "text"
      },
      "source": [
        "Let's implement it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOH703-Zlofi",
        "colab_type": "text"
      },
      "source": [
        "We need to add top_k parameter in generate function to use top-k sampling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NoAxdcZObHC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "5a09ec67-ec22-4ffd-901c-1a149f1fdba4"
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output2 = model.generate(\n",
        "    ids1, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output2[0], skip_special_tokens=True))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[ WP ] The King must leave the throne now. <endprompts>\n",
            "\n",
            "LAPISTA:\n",
            "But, lord, do give me some time so your presence can be taken\n",
            "and we both be gone: by my grace will I go\n",
            "from that point; what would you say, though you\n",
            "may't\n",
            "Have heard it so far?\n",
            "\n",
            "NORFOLK:\n",
            "And let's come back again.\n",
            "\n",
            "PRIO:\n",
            "Good day to you, good morning.\n",
            "You'll do well to hear from me again: it's a pleasure\n",
            "to hear what you'll be doing at this time.\n",
            "And how canthither have you said I went to see you and\n",
            "your wife?\n",
            "\n",
            "TOMAS:\n",
            "That thou art so good about thy mother's life that she should seem\n",
            "like a lady to me too; for that we are now friends,\n",
            "she needs not for life.\n",
            "\n",
            "LAPISTA:\n",
            "Good, good sir; and look forward to her coming hither.\n",
            "\n",
            "PRIO:\n",
            "Now, she comes too late.\n",
            "\n",
            "LAPISTA:\n",
            "Why, good, she comes too late; she is too\n",
            "short of breath; she is quite young; and indeed,\n",
            "she cannot breathe.\n",
            "\n",
            "PRIO:\n",
            "Why, good sir, she comes too late; her tongue\n",
            "breathes too much. O, what have I done, O?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9oiwPsjl3n5",
        "colab_type": "text"
      },
      "source": [
        "Now, after implementing top-k sampling, we should try out top-p sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-aYt_FOUTx6",
        "colab_type": "text"
      },
      "source": [
        "## Top-p (nucleus) sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M75hpAZeUshq",
        "colab_type": "text"
      },
      "source": [
        "It is selecting the highest probability tokens whose cumulative probability mass\n",
        "exceeds the pre-chosen threshold p."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQzKSp3fUQ1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "ad086e91-3f8c-4bc6-f626-c29892efaa92"
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output3 = model.generate(\n",
        "    ids1, \n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_p=0.92,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output3[0], skip_special_tokens=True))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[ WP ] The King must leave the throne now. <endprompts>\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "I do not think so.\n",
            "\n",
            "GLOUCESTER:\n",
            "No.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "I cannot, but will. I shall, for the king.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king must leave the throne now, as I have said.\n",
            "\n",
            "GLOUCESTER:\n",
            "There is no more king left in the house.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "But, by the grace of God, I am satisfied with him.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king will leave the throne, as I have said,\n",
            "as he had done it before: for to do this, he had promised,\n",
            "before. I will go to the king and be content,\n",
            "and by my power be gone.\n",
            "\n",
            "GLOUCESTER:\n",
            "No more king left. There is no king left in the house.\n",
            "\n",
            "GLOUCESTER:\n",
            "The king shall leave the throne now.\n",
            "\n",
            "GLOUCESTER:\n",
            "No more king left. There is no king left in the house.\n",
            "\n",
            "GLOUCESTER:\n",
            "No more king left. There is no king left in the house.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb8kBnOwVGfk",
        "colab_type": "text"
      },
      "source": [
        "It's time to combine everything we did previously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJPuQuBUQyD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "f58f9883-c47c-403d-cbbd-3c674a483725"
      },
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 \n",
        "final_outputs = model.generate(\n",
        "    ids1,\n",
        "    do_sample=True, \n",
        "    max_length=300, \n",
        "    top_k=40, \n",
        "    top_p=0.95, \n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, final_output in enumerate(final_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(final_output, skip_special_tokens=True)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: [ WP ] The King must leave the throne now. <endprompts>\n",
            "\n",
            "TRANIO:\n",
            "The Duke of York,\n",
            "How dost thou tell me it?\n",
            "\n",
            "KING RICHARD II:\n",
            "He that hath his crown of York is but one of them\n",
            "That hath no crown of York.\n",
            "\n",
            "TRANIO:\n",
            "His crown, therefore, is no crown of York.\n",
            "\n",
            "KING RICHARD II:\n",
            "For he that hath his crown hath no crown of York.\n",
            "\n",
            "TRANIO:\n",
            "\n",
            "King Richard is his son, and so shall he.\n",
            "\n",
            "KING RICHARD II:\n",
            "What hath your father's son been born to thee?\n",
            "\n",
            "LUCENTIO:\n",
            "The prince died, as I say;\n",
            "My father, as I say, died to me;\n",
            "And if the Duke of York should wish his son for the prince,\n",
            "For that I was not his son,\n",
            "The boy was made to stand by him.\n",
            "\n",
            "TRANIO:\n",
            "What then?\n",
            "\n",
            "LUCENTIO:\n",
            "To have the father's son be crowned by him,\n",
            "And, like him, give him to the prince.\n",
            "\n",
            "KING RICHARD II:\n",
            "What then, good Lucentio?\n",
            "\n",
            "LUCENTIO:\n",
            "A daughter to the Duke of York,\n",
            "That he may call my son Lucentio.\n",
            "\n",
            "TRANIO:\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo-SKcjYVz7R",
        "colab_type": "text"
      },
      "source": [
        "This is the final output text we generated and it tries to show order of events. The parameters can be tuned further to get better results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uppEXzyKWVwT",
        "colab_type": "text"
      },
      "source": [
        "## Thanks for reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmeDsZgJWcp0",
        "colab_type": "text"
      },
      "source": [
        "I referred the following links to make this an easy tutorial. You can go through these if want to go in depth.\n",
        "\n",
        "https://huggingface.co/blog/how-to-generate\n",
        "\n",
        "https://arxiv.org/pdf/1904.09751.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvWas92NUQwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWc7bWqSG2N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62az2uTXG1_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_nRciuCG19n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}